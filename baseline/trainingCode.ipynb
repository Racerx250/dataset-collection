{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_icdataset_train_test() got an unexpected keyword argument 'use_int_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aa91941a2d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mic_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_icdataset_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/dataset_stanford_dog_recreation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_int_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;31m#valid_dataset = dogData.Dog_Test_Dataset('val_list.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m#test_dataset = datasets.ImageFolder()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_icdataset_train_test() got an unexpected keyword argument 'use_int_labels'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dogData\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.models as models\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision import datasets as datasets\n",
    "import ic_dataset\n",
    "\n",
    "TEST_MODEL = False\n",
    "EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "\n",
    "# xavier initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# utility function for plotting\n",
    "def graph(val_loss, train_loss, label):\n",
    "    min_epoch = len(val_loss)\n",
    "\n",
    "    x = [[i + 1] for i in range(min_epoch)]\n",
    "\n",
    "    if label == \"loss\":\n",
    "        plt.plot(x, train_loss, alpha=0.9, linewidth=2.5, color='red', label=\"train_loss\")\n",
    "        plt.plot(x, val_loss, alpha=0.8, color='blue', linewidth=1.5, label=\"validate loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "    if label == \"acc\":\n",
    "        plt.plot(x, train_loss, alpha=0.9, linewidth=2.5, color='red', label=\"train_acc\")\n",
    "        plt.plot(x, val_loss, alpha=0.8, color='blue', linewidth=1.5, label=\"validate_acc\")\n",
    "        plt.ylabel(\"acc\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"epoch size\")\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, dataloaders, epochs, optimizer, criterion, patience, test_model) :\n",
    "    train_dataloader = dataloaders['train']\n",
    "    val_dataloader = dataloaders['validate']\n",
    "    if (test_model) :\n",
    "        test_dataloader = dataloaders['test']\n",
    "    loss_increase = 0\n",
    "    train_loss_total = []\n",
    "    val_loss_total = []\n",
    "    train_acc_total = []\n",
    "    valid_acc_total = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        batch = 0\n",
    "        batch_valid = 0\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        # train\n",
    "        for i, sample in enumerate(train_dataloader) :\n",
    "            batch += 1\n",
    "            image, label = sample\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(image)\n",
    "            # get train accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_acc += torch.sum(predicted == label)\n",
    "            # get train loss\n",
    "            loss = criterion(output, label)\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(\"epoch train complete\")\n",
    "\n",
    "        # validate \n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in enumerate(val_dataloader) :\n",
    "                batch_valid += 1\n",
    "                image, label = sample\n",
    "                image = image.cuda()\n",
    "                label = label.cuda()\n",
    "                # validate accuracy, loss\n",
    "                output_valid = model(image)\n",
    "                _, predicted = torch.max(output_valid.data, 1)\n",
    "                valid_acc += torch.sum(predicted == label)\n",
    "                loss = criterion(output_valid, label)\n",
    "                val_loss += loss.item()\n",
    "        model = model.train()\n",
    "\n",
    "        print(\"epoch validate complete\")\n",
    "\n",
    "        train_loss_total.append(train_loss / batch)\n",
    "        val_loss_total.append(val_loss / batch_valid)\n",
    "\n",
    "        train_acc_total.append(train_acc.cpu().numpy() / len(train_dataloader.dataset))\n",
    "        valid_acc_total.append(valid_acc.cpu().numpy() / len(val_dataloader.dataset))\n",
    "\n",
    "        if (epoch != 0) and (val_loss_total[epoch] >= val_loss_total[epoch - 1]):\n",
    "            loss_increase += 1\n",
    "            if loss_increase >= patience :\n",
    "                #torch.save(model.state_dict(), './')\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, \"./\")\n",
    "\n",
    "                print (\"early stopping\")\n",
    "                break\n",
    "                \n",
    "        else :\n",
    "            loss_increase = 0\n",
    "\n",
    "    # graph train/validation loss and accuracy\n",
    "    graph(val_loss_total, train_loss_total, \"loss\")\n",
    "    graph(valid_acc_total, train_acc_total, \"acc\")\n",
    "    \n",
    "    # save model\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, \"./\")\n",
    "\n",
    "    if (test_model) :\n",
    "        test_acc = 0\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, test in enumerate(test_dataloader):\n",
    "                images, labels = test\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_acc += torch.sum(predicted == labels)\n",
    "        print(\"Accuracy of the network: \" + str(test_acc.cpu().numpy()/len(test_dataset)))\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    model = models.inception_v3(pretrained=False, init_weights=True)\n",
    "    model.fc = nn.Linear(2048, 120)\n",
    "   # model.fc = nn.Linear(512,120)\n",
    "   # model.fc.apply(weights_init)\n",
    "\n",
    "   # model.classifier[6] = nn.Linear(4096,120)\n",
    "   # model.classifier[6].apply(weights_init)\n",
    "\n",
    "    \n",
    "    # load the datasets\n",
    "    \n",
    "    train_dataset, valid_dataset = ic_dataset.get_icdataset_train_test('/data/dataset_stanford_dog_recreation', train_perc=.8, use_int_labels=True)\n",
    "    #valid_dataset = dogData.Dog_Test_Dataset('val_list.txt')\n",
    "    #test_dataset = datasets.ImageFolder()\n",
    "    \n",
    "    \"\"\"\n",
    "    train_transform = transforms.Compose([ \n",
    "        transforms.Resize(256), \n",
    "        transforms.RandomCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    test_transform = transforms.Compose([ \n",
    "        transforms.Resize(256), \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \"\"\"\n",
    "    #train_dataset= datasets.ImageFolder('TrainImages/', transform=train_transform)\n",
    "    #valid_dataset= datasets.ImageFolder('ValImages/', transform=test_transform)\n",
    "    #test_dataset= datasets.ImageFolder('TestImages/', transform=test_transform)\n",
    "\n",
    "    #train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(total_dataset, [14406, 3087, 3087])\n",
    "\n",
    "    # dataloaders for the datasets\n",
    "    dataloaders = defaultdict(DataLoader)\n",
    "    dataloaders['train'] =  DataLoader(train_dataset, shuffle=True, batch_size = 64, num_workers=4)\n",
    "    dataloaders['validate'] =  DataLoader(valid_dataset, shuffle=False, num_workers=4)\n",
    "    if (TEST_MODEL):\n",
    "        dataloaders['test'] =  DataLoader(test_dataset, shuffle=False, num_workers=4)\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.cuda()\n",
    "    train_model(model, dataloaders, EPOCHS, optimizer, criterion, PATIENCE, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
